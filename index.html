<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Call Assistant</title>
    <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-color: #0c0d12;
            --accent-color: #7c4dff;
            --danger-color: #ff5252;
            --success-color: #00e676;
            --text-primary: #ffffff;
            --glass-bg: rgba(255, 255, 255, 0.05);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Outfit', sans-serif;
        }

        body {
            background: var(--bg-color);
            background: radial-gradient(circle at center, #1a1b26 0%, #0c0d12 100%);
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            overflow: hidden;
            color: var(--text-primary);
        }

        .call-container {
            width: 100%;
            max-width: 450px;
            height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: space-between;
            padding: 80px 20px;
            text-align: center;
            position: relative;
        }

        /* Avatar & Pulsing Effect */
        .avatar-container {
            position: relative;
            margin-top: 20px;
        }

        .avatar {
            width: 180px;
            height: 180px;
            background: linear-gradient(135deg, var(--accent-color), #40c4ff);
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 4rem;
            z-index: 2;
            position: relative;
            box-shadow: 0 0 50px rgba(124, 77, 255, 0.3);
            transition: all 0.5s ease;
        }

        .pulse {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 180px;
            height: 180px;
            background: var(--accent-color);
            border-radius: 50%;
            opacity: 0;
            transition: opacity 0.3s;
        }

        .active-call .pulse {
            opacity: 0.3;
            animation: pulse-out 2s infinite ease-out;
        }

        @keyframes pulse-out {
            0% {
                transform: translate(-50%, -50%) scale(1);
                opacity: 0.5;
            }

            100% {
                transform: translate(-50%, -50%) scale(2.5);
                opacity: 0;
            }
        }

        /* Status & Text */
        .call-status {
            font-size: 1.1rem;
            font-weight: 300;
            color: rgba(255, 255, 255, 0.6);
            letter-spacing: 3px;
            text-transform: uppercase;
        }

        .ai-name {
            font-size: 2.5rem;
            font-weight: 600;
            margin-top: 10px;
        }

        .subtitle {
            font-size: 1rem;
            color: var(--accent-color);
            margin-top: 20px;
            height: 1.5em;
            font-weight: 400;
        }

        /* Controls */
        .controls {
            display: flex;
            flex-direction: column;
            gap: 20px;
            margin-bottom: 50px;
            width: 100%;
            align-items: center;
        }

        .btn {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            display: flex;
            justify-content: center;
            align-items: center;
            transition: all 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            color: white;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
        }

        .btn-call {
            background: var(--success-color);
        }

        .btn-end {
            background: var(--danger-color);
            display: none;
        }

        .btn:hover {
            transform: scale(1.1);
        }

        .btn:active {
            transform: scale(0.9);
        }

        .btn svg {
            width: 35px;
            height: 35px;
            fill: currentColor;
        }

        /* transcription for debugging */
        #transcription {
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.4);
            max-width: 80%;
            margin-top: 20px;
            min-height: 1.2em;
        }

        /* Call Screen overlay logic */
        .call-container.dialing .avatar {
            transform: scale(0.9);
            opacity: 0.7;
        }

        .call-container.connected .avatar {
            transform: scale(1);
            opacity: 1;
        }
    </style>
</head>

<body>

    <div class="call-container" id="main-container">
        <div class="top-info">
            <div class="call-status" id="status">Standby</div>
            <div class="ai-name">Sensi Voice</div>
        </div>

        <div class="avatar-container" id="avatar-box">
            <div class="pulse"></div>
            <div class="avatar">ðŸ¤–</div>
            <div class="subtitle" id="subtitle-text">Ready to start</div>
            <div id="transcription"></div>
        </div>

        <div class="controls">
            <button id="btn-start" class="btn btn-call" title="Start Call">
                <svg viewBox="0 0 24 24">
                    <path
                        d="M6.62,10.79C8.06,13.62 10.38,15.94 13.21,17.38L15.41,15.18C15.69,14.9 16.08,14.82 16.43,14.93C17.55,15.3 18.75,15.5 20,15.5A1,1 0 0,1 21,16.5V20A1,1 0 0,1 20,21A17,17 0 0,1 3,4A1,1 0 0,1 4,3H7.5A1,1 0 0,1 8.5,4C8.5,5.25 8.7,6.45 9.07,7.57C9.18,7.92 9.1,8.31 8.82,8.59L6.62,10.79Z" />
                </svg>
            </button>
            <button id="btn-end" class="btn btn-end" title="End Call">
                <svg viewBox="0 0 24 24">
                    <path
                        d="M12,9C10.4,9 8.85,9.25 7.4,9.72V12.82C7.4,13.22 7.17,13.56 6.84,13.72C5.86,14.21 4.97,14.84 4.17,15.57C4,15.75 3.75,15.86 3.5,15.86C3.2,15.86 2.95,15.74 2.77,15.56L0.29,13.08C0.11,12.9 0,12.65 0,12.38C0,12.1 0.11,11.85 0.29,11.67C3.38,8.77 7.49,7 12,7C16.51,7 20.62,8.77 23.71,11.67C23.89,11.85 24,12.1 24,12.38C24,12.65 23.89,12.9 23.71,13.08L21.23,15.56C21.05,15.74 20.8,15.86 20.5,15.86C20.25,15.86 20,15.75 19.82,15.57C19.03,14.84 18.14,14.21 17.16,13.72C16.83,13.56 16.6,13.22 16.6,12.82V9.72C15.15,9.25 13.6,9 12,9Z" />
                </svg>
            </button>
        </div>
    </div>

    <script>
        const CONFIG = {
            N8N_URL: 'https://n8n.srv962022.hstgr.cloud/webhook/sarvam'
        };

        const STATE = {
            isActive: false,
            isSpeaking: false,
            audioQueue: [],
            recognition: null,
            audioCtx: null,
            analyser: null,
            micStream: null,
            interruptThreshold: 45, // Increased back to prevent noise sensitivity
            aiStartTime: 0,
            allowBargeInAfter: 1200, // Balanced grace period
            volumeLevel: 0
        };

        const DOM = {
            container: document.getElementById('main-container'),
            status: document.getElementById('status'),
            subtitle: document.getElementById('subtitle-text'),
            startBtn: document.getElementById('btn-start'),
            endBtn: document.getElementById('btn-end')
        };

        // --- Audio Context & Dial Tone ---
        function getAudioCtx() {
            if (!STATE.audioCtx) {
                STATE.audioCtx = new (window.AudioContext || window.webkitAudioContext)({
                    latencyHint: 'interactive'
                });
            }
            if (STATE.audioCtx.state === 'suspended') STATE.audioCtx.resume();
            return STATE.audioCtx;
        }

        let dialInterval;
        function startDialTone() {
            const ctx = getAudioCtx();
            const tone = () => {
                const osc1 = ctx.createOscillator();
                const osc2 = ctx.createOscillator();
                const gain = ctx.createGain();
                osc1.frequency.value = 440;
                osc2.frequency.value = 480;
                gain.gain.value = 0.05;
                osc1.connect(gain); osc2.connect(gain);
                gain.connect(ctx.destination);
                osc1.start(); osc2.start();
                setTimeout(() => { osc1.stop(); osc2.stop(); }, 2000);
            };
            tone();
            dialInterval = setInterval(tone, 4000);
        }

        function stopDialTone() {
            clearInterval(dialInterval);
        }

        // --- Interruption Logic (Barge-In) ---
        function handleUserInterrupt() {
            const now = Date.now();
            if (STATE.isSpeaking && (now - STATE.aiStartTime > STATE.allowBargeInAfter)) {
                console.log("Barge-in detected!");
                interruptAI();
            }
        }

        function interruptAI() {
            if (STATE.isSpeaking || STATE.audioQueue.length > 0) {
                STATE.audioQueue.forEach(a => {
                    try { a.pause(); a.src = ""; } catch (e) { }
                });
                STATE.audioQueue = [];
                STATE.isSpeaking = false;
                DOM.subtitle.textContent = "Listening...";
                DOM.status.textContent = "Connected";
            }
        }

        // --- Speech Detection (Volume Analysis) ---
        function detectSpeech() {
            if (!STATE.isActive || !STATE.analyser) return;

            const dataArray = new Uint8Array(STATE.analyser.frequencyBinCount);
            STATE.analyser.getByteFrequencyData(dataArray);

            const volume = dataArray.reduce((a, b) => a + b) / dataArray.length;
            STATE.volumeLevel = volume;

            // Visual feedback - less sensitive to avoid phantom flashing
            if (volume > 15) {
                DOM.status.style.color = (volume > STATE.interruptThreshold) ? "#ff5252" : "#00e676";
                DOM.status.style.opacity = 1;
            } else {
                DOM.status.style.color = "white";
                DOM.status.style.opacity = 0.8;
            }

            // Only barge-in if AI is speaking and volume crosses threshold
            if (volume > STATE.interruptThreshold && STATE.isSpeaking) {
                handleUserInterrupt();
            }

            if (STATE.isActive) {
                requestAnimationFrame(detectSpeech);
            }
        }

        // --- Speech Recognition ---
        async function startListening() {
            try {
                // IMPORTANT: Start Recognition BEFORE getUserMedia to avoid hardware lock/conflict
                const SpeechRec = window.SpeechRecognition || window.webkitSpeechRecognition;
                if (!SpeechRec) {
                    DOM.status.textContent = "Browser Not Supported";
                    return;
                }

                STATE.recognition = new SpeechRec();
                STATE.recognition.continuous = true;
                STATE.recognition.interimResults = true;
                STATE.recognition.lang = 'en-IN';

                STATE.recognition.onstart = async () => {
                    console.log("STT Active");
                    DOM.status.textContent = "Connected";

                    // Now that Recognition has fixed the hardware channel, start the Analyser
                    try {
                        const micStream = await navigator.mediaDevices.getUserMedia({
                            audio: { echoCancellation: true, autoGainControl: true }
                        });
                        STATE.micStream = micStream;
                        const ctx = getAudioCtx();
                        const source = ctx.createMediaStreamSource(micStream);
                        STATE.analyser = ctx.createAnalyser();
                        STATE.analyser.fftSize = 256;
                        source.connect(STATE.analyser);
                        detectSpeech();
                    } catch (e) {
                        console.warn("Analyser blocked, but STT should still work.");
                    }
                };

                STATE.recognition.onerror = (e) => {
                    console.error("STT Error:", e.error);
                    if (e.error === 'network') DOM.status.textContent = "Network Error";
                    if (e.error === 'not-allowed') DOM.status.textContent = "Mic Denied";

                    if (STATE.isActive && e.error !== 'not-allowed') {
                        setTimeout(() => { if (STATE.isActive) try { STATE.recognition.start(); } catch (err) { } }, 1000);
                    }
                };

                STATE.recognition.onresult = (event) => {
                    const result = event.results[event.results.length - 1];
                    const text = result[0].transcript;

                    // Show transcription in the MAIN label for the user
                    if (text.trim().length > 0) {
                        DOM.subtitle.textContent = text;
                        DOM.subtitle.style.color = "white";
                    }

                    // If AI is talking -> interrupt if user spoke more than a tiny bit
                    if (STATE.isSpeaking && text.length > 5) {
                        interruptAI();
                    }

                    if (result.isFinal && text.trim().length > 1) {
                        sendToAI(text);
                    }
                };

                STATE.recognition.onend = () => {
                    if (STATE.isActive) {
                        try { STATE.recognition.start(); } catch (e) { }
                    }
                };

                STATE.recognition.start();

            } catch (err) {
                console.error("Audio Start Failure:", err);
                DOM.status.textContent = "System Error";
            }
        }

        async function sendToAI(text) {
            if (!STATE.isActive) return;
            DOM.status.textContent = "Thinking...";
            try {
                const res = await fetch(CONFIG.N8N_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ question: text })
                });
                const data = await res.json();
                const item = Array.isArray(data) ? data[0] : data;

                if (item && (item.audios || item.data)) {
                    playVoice(item.audios || [item.data]);
                } else {
                    DOM.status.textContent = "Connected";
                    DOM.subtitle.textContent = "I'm listening...";
                }
            } catch (e) {
                console.error("Webhook Error:", e);
                DOM.status.textContent = "CORS/Server Error";
                setTimeout(() => { DOM.status.textContent = "Connected"; }, 2000);
            }
        }

        async function playVoice(chunks) {
            const ctx = getAudioCtx();
            interruptAI();
            STATE.isSpeaking = true;
            STATE.aiStartTime = Date.now();
            DOM.status.textContent = "Speaking";
            DOM.subtitle.textContent = "Assistant is responding...";
            DOM.subtitle.style.color = "var(--accent-color)";

            for (let b64 of chunks) {
                if (!STATE.isSpeaking) break;
                await new Promise(resolve => {
                    try {
                        const audio = new Audio("data:audio/mp3;base64," + (b64.includes(',') ? b64.split(',')[1] : b64));
                        const source = ctx.createMediaElementSource(audio);
                        source.connect(ctx.destination);

                        STATE.audioQueue.push(audio);
                        audio.onended = () => {
                            STATE.audioQueue = STATE.audioQueue.filter(a => a !== audio);
                            resolve();
                        };
                        audio.onerror = resolve;
                        audio.play().catch(resolve);
                    } catch (e) {
                        resolve();
                    }
                });
            }

            if (STATE.isActive && STATE.audioQueue.length === 0) {
                STATE.isSpeaking = false;
                DOM.status.textContent = "Connected";
                DOM.subtitle.textContent = "I'm listening...";
                DOM.subtitle.style.color = "rgba(255,255,255,0.7)";
            }
        }

        // --- Call Lifecycle ---
        DOM.startBtn.addEventListener('click', () => {
            getAudioCtx();
            STATE.isActive = true;
            DOM.container.classList.add('active-call', 'dialing');
            DOM.startBtn.style.display = 'none';
            DOM.endBtn.style.display = 'flex';
            DOM.status.textContent = "Dialing...";

            startDialTone();

            // Simulate pickup
            setTimeout(() => {
                if (!STATE.isActive) return;
                stopDialTone();
                DOM.container.classList.remove('dialing');
                DOM.container.classList.add('connected');
                DOM.status.textContent = "Connected";
                DOM.subtitle.textContent = "Listening...";
                startListening();
                // Handshake with AI
                sendToAI("Hello, are you there?");
            }, 3000);
        });

        DOM.endBtn.addEventListener('click', () => {
            STATE.isActive = false;
            stopDialTone();
            interruptAI();

            if (STATE.recognition) {
                STATE.recognition.onend = null;
                STATE.recognition.stop();
            }

            if (STATE.micStream) {
                STATE.micStream.getTracks().forEach(track => track.stop());
            }

            DOM.container.classList.remove('active-call', 'dialing', 'connected');
            DOM.startBtn.style.display = 'flex';
            DOM.endBtn.style.display = 'none';
            DOM.status.textContent = "Standby";
            DOM.subtitle.textContent = "Ready to start";
            DOM.transcription.textContent = "";
        });

    </script>
</body>

</html>
