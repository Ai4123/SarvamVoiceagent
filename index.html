<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Call Assistant</title>
    <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg-color: #0c0d12;
            --accent-color: #7c4dff;
            --danger-color: #ff5252;
            --success-color: #00e676;
            --text-primary: #ffffff;
            --glass-bg: rgba(255, 255, 255, 0.05);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Outfit', sans-serif;
        }

        body {
            background: var(--bg-color);
            background: radial-gradient(circle at center, #1a1b26 0%, #0c0d12 100%);
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            overflow: hidden;
            color: var(--text-primary);
        }

        .call-container {
            width: 100%;
            max-width: 450px;
            height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: space-between;
            padding: 80px 20px;
            text-align: center;
            position: relative;
        }

        .avatar-container {
            position: relative;
            margin-top: 20px;
        }

        .avatar {
            width: 180px;
            height: 180px;
            background: linear-gradient(135deg, var(--accent-color), #40c4ff);
            border-radius: 50%;
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 4rem;
            z-index: 2;
            position: relative;
            box-shadow: 0 0 50px rgba(124, 77, 255, 0.3);
            transition: all 0.5s ease;
        }

        .pulse {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 180px;
            height: 180px;
            background: var(--accent-color);
            border-radius: 50%;
            opacity: 0;
            transition: opacity 0.3s;
        }

        .active-call .pulse {
            opacity: 0.3;
            animation: pulse-out 2s infinite ease-out;
        }

        @keyframes pulse-out {
            0% {
                transform: translate(-50%, -50%) scale(1);
                opacity: 0.5;
            }

            100% {
                transform: translate(-50%, -50%) scale(2.5);
                opacity: 0;
            }
        }

        .call-status {
            font-size: 1.1rem;
            font-weight: 300;
            color: rgba(255, 255, 255, 0.6);
            letter-spacing: 3px;
            text-transform: uppercase;
        }

        .ai-name {
            font-size: 2.5rem;
            font-weight: 600;
            margin-top: 10px;
        }

        .subtitle {
            font-size: 1rem;
            color: var(--accent-color);
            margin-top: 20px;
            height: 1.5em;
            font-weight: 400;
        }

        .controls {
            display: flex;
            flex-direction: column;
            gap: 20px;
            margin-bottom: 50px;
            width: 100%;
            align-items: center;
        }

        .btn {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            display: flex;
            justify-content: center;
            align-items: center;
            transition: all 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            color: white;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
        }

        .btn-call {
            background: var(--success-color);
        }

        .btn-end {
            background: var(--danger-color);
            display: none;
        }

        .btn:hover {
            transform: scale(1.1);
        }

        .btn:active {
            transform: scale(0.9);
        }

        .btn svg {
            width: 35px;
            height: 35px;
            fill: currentColor;
        }

        #transcription {
            font-size: 0.9rem;
            color: rgba(255, 255, 255, 0.4);
            max-width: 80%;
            margin-top: 20px;
            min-height: 1.2em;
        }

        .call-container.dialing .avatar {
            transform: scale(0.9);
            opacity: 0.7;
        }

        .call-container.connected .avatar {
            transform: scale(1);
            opacity: 1;
        }
    </style>
</head>

<body>

    <div class="call-container" id="main-container">
        <div class="top-info">
            <div class="call-status" id="status">Standby</div>
            <div class="ai-name">Sensi Voice</div>
        </div>

        <div class="avatar-container" id="avatar-box">
            <div class="pulse"></div>
            <div class="avatar"></div>
            <div class="subtitle" id="subtitle-text">Ready to start</div>
            <div id="transcription"></div>
        </div>

        <div class="controls">
            <button id="btn-start" class="btn btn-call" title="Start Call">
                <svg viewBox="0 0 24 24">
                    <path
                        d="M6.62,10.79C8.06,13.62 10.38,15.94 13.21,17.38L15.41,15.18C15.69,14.9 16.08,14.82 16.43,14.93C17.55,15.3 18.75,15.5 20,15.5A1,1 0 0,1 21,16.5V20A1,1 0 0,1 20,21A17,17 0 0,1 3,4A1,1 0 0,1 4,3H7.5A1,1 0 0,1 8.5,4C8.5,5.25 8.7,6.45 9.07,7.57C9.18,7.92 9.1,8.31 8.82,8.59L6.62,10.79Z" />
                </svg>
            </button>
            <button id="btn-end" class="btn btn-end" title="End Call">
                <svg viewBox="0 0 24 24">
                    <path
                        d="M12,9C10.4,9 8.85,9.25 7.4,9.72V12.82C7.4,13.22 7.17,13.56 6.84,13.72C5.86,14.21 4.97,14.84 4.17,15.57C4,15.75 3.75,15.86 3.5,15.86C3.2,15.86 2.95,15.74 2.77,15.56L0.29,13.08C0.11,12.9 0,12.65 0,12.38C0,12.1 0.11,11.85 0.29,11.67C3.38,8.77 7.49,7 12,7C16.51,7 20.62,8.77 23.71,11.67C23.89,11.85 24,12.1 24,12.38C24,12.65 23.89,12.9 23.71,13.08L21.23,15.56C21.05,15.74 20.8,15.86 20.5,15.86C20.25,15.86 20,15.75 19.82,15.57C19.03,14.84 18.14,14.21 17.16,13.72C16.83,13.56 16.6,13.22 16.6,12.82V9.72C15.15,9.25 13.6,9 12,9Z" />
                </svg>
            </button>
        </div>
    </div>

    <script>
        // â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        const CONFIG = {
            N8N_URL: 'https://n8n.srv962022.hstgr.cloud/webhook/voice',
            SARVAM_KEY: 'sk_2g2f57ke_5Gud6MDIcIPqC36rgbs7ezLd',
            SARVAM_URL: 'https://api.sarvam.ai/text-to-speech',
            PREWARM_INTERVAL_MS: 4 * 60 * 1000,
            SILENCE_THRESHOLD_MS: 350,
            MIN_WORD_COUNT: 2,
            MIN_CONFIDENCE: 0.4
        };

        // â”€â”€â”€ CACHE & HISTORY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        const responseCache = new Map();
        function cacheKey(q) { return q.toLowerCase().replace(/[^a-z0-9 ]/g, '').trim(); }

        let conversationHistory = [];
        const MAX_HISTORY_TURNS = 4;

        // â”€â”€â”€ PRE-WARM n8n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        function prewarmN8N() {
            fetch(CONFIG.N8N_URL, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ question: '__prewarm__' })
            }).then(() => console.log(' n8n pre-warmed')).catch(() => { });
        }
        prewarmN8N();
        setInterval(prewarmN8N, CONFIG.PREWARM_INTERVAL_MS);

        // â”€â”€â”€ STATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        const STATE = {
            isActive: false,
            isSpeaking: false,
            audioQueue: [],
            recognition: null,
            audioCtx: null,
            analyser: null,
            micStream: null,
            interruptThreshold: 45,
            aiStartTime: 0,
            allowBargeInAfter: 1200,
            volumeLevel: 0,
            sttRunning: false,
            suppressSTT: false,
            silenceTimer: null,
            lastInterimText: ''
        };

        // â”€â”€â”€ DOM REFS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        const DOM = {
            container: document.getElementById('main-container'),
            status: document.getElementById('status'),
            subtitle: document.getElementById('subtitle-text'),
            startBtn: document.getElementById('btn-start'),
            endBtn: document.getElementById('btn-end'),
            transcription: document.getElementById('transcription')
        };

        // â”€â”€â”€ AUDIO CONTEXT & DIAL TONE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        function getAudioCtx() {
            if (!STATE.audioCtx) {
                STATE.audioCtx = new (window.AudioContext || window.webkitAudioContext)({
                    latencyHint: 'interactive'
                });
            }
            if (STATE.audioCtx.state === 'suspended') STATE.audioCtx.resume();
            return STATE.audioCtx;
        }

        let dialInterval;
        function startDialTone() {
            const ctx = getAudioCtx();
            const tone = () => {
                const osc1 = ctx.createOscillator();
                const osc2 = ctx.createOscillator();
                const gain = ctx.createGain();
                osc1.frequency.value = 440;
                osc2.frequency.value = 480;
                gain.gain.value = 0.05;
                osc1.connect(gain); osc2.connect(gain);
                gain.connect(ctx.destination);
                osc1.start(); osc2.start();
                setTimeout(() => { osc1.stop(); osc2.stop(); }, 2000);
            };
            tone();
            dialInterval = setInterval(tone, 4000);
        }

        function stopDialTone() { clearInterval(dialInterval); }

        // â”€â”€â”€ STT PAUSE / RESUME (Echo Prevention) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        function pauseSTT() {
            if (STATE.recognition && STATE.sttRunning) {
                STATE.suppressSTT = true;
                try { STATE.recognition.stop(); } catch (e) { }
                STATE.sttRunning = false;
                console.log(" STT paused (AI speaking)");
            }
        }

        function resumeSTT() {
            STATE.suppressSTT = false;
            setTimeout(() => {
                if (STATE.isActive && !STATE.sttRunning && !STATE.isSpeaking) {
                    try {
                        STATE.recognition.start();
                        console.log("ðŸŽ¤ STT resumed");
                    } catch (e) { }
                }
            }, 400);
        }

        function clearSilenceTimer() {
            if (STATE.silenceTimer) {
                clearTimeout(STATE.silenceTimer);
                STATE.silenceTimer = null;
            }
        }

        // â”€â”€â”€ INTERRUPTION / BARGE-IN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        function handleUserInterrupt() {
            const now = Date.now();
            if (STATE.isSpeaking && (now - STATE.aiStartTime > STATE.allowBargeInAfter)) {
                console.log(" Barge-in detected!");
                interruptAI();
            }
        }

        function interruptAI() {
            if (STATE.isSpeaking || STATE.audioQueue.length > 0) {
                STATE.audioQueue.forEach(a => {
                    try { a.pause(); a.src = ""; } catch (e) { }
                });
                STATE.audioQueue = [];
                STATE.isSpeaking = false;
                DOM.subtitle.textContent = "Listening...";
                DOM.status.textContent = "Connected";
                resumeSTT();
            }
        }

        // â”€â”€â”€ VOLUME ANALYSIS (for barge-in only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        function detectSpeech() {
            if (!STATE.isActive || !STATE.analyser) return;

            const dataArray = new Uint8Array(STATE.analyser.frequencyBinCount);
            STATE.analyser.getByteFrequencyData(dataArray);

            const volume = dataArray.reduce((a, b) => a + b) / dataArray.length;
            STATE.volumeLevel = volume;

            if (volume > 15) {
                DOM.status.style.color = (volume > STATE.interruptThreshold) ? "#ff5252" : "#00e676";
                DOM.status.style.opacity = 1;
            } else {
                DOM.status.style.color = "white";
                DOM.status.style.opacity = 0.8;
            }

            if (volume > STATE.interruptThreshold && STATE.isSpeaking) {
                handleUserInterrupt();
            }

            if (STATE.isActive) requestAnimationFrame(detectSpeech);
        }

        // â”€â”€â”€ NOISE GATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        function isNoise(text, confidence) {
            const words = text.split(/\s+/).filter(w => w.length > 1);
            if (words.length < CONFIG.MIN_WORD_COUNT) {
                console.log(' Noise [words]:', text);
                return true;
            }
            if (confidence > 0 && confidence < CONFIG.MIN_CONFIDENCE) {
                console.log(' Noise [conf]:', confidence.toFixed(2), text);
                return true;
            }
            return false;
        }

        // â”€â”€â”€ SPEECH RECOGNITION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        async function startListening() {
            try {
                const SpeechRec = window.SpeechRecognition || window.webkitSpeechRecognition;
                if (!SpeechRec) {
                    DOM.status.textContent = "Browser Not Supported";
                    return;
                }

                STATE.recognition = new SpeechRec();
                STATE.recognition.continuous = true;
                STATE.recognition.interimResults = true;
                STATE.recognition.lang = 'en-IN';

                STATE.recognition.onstart = async () => {
                    console.log(" STT Active");
                    STATE.sttRunning = true;
                    DOM.status.textContent = "Connected";

                    if (!STATE.analyser) {
                        try {
                            const micStream = await navigator.mediaDevices.getUserMedia({
                                audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true }
                            });
                            STATE.micStream = micStream;
                            const ctx = getAudioCtx();
                            const source = ctx.createMediaStreamSource(micStream);
                            STATE.analyser = ctx.createAnalyser();
                            STATE.analyser.fftSize = 256;
                            source.connect(STATE.analyser);
                            detectSpeech();
                        } catch (e) {
                            console.warn("Analyser blocked, barge-in via volume won't work.");
                        }
                    }
                };

                STATE.recognition.onerror = (e) => {
                    console.error("STT Error:", e.error);
                    STATE.sttRunning = false;
                    if (e.error === 'network') DOM.status.textContent = "Network Error";
                    if (e.error === 'not-allowed') DOM.status.textContent = "Mic Denied";

                    if (STATE.isActive && !STATE.suppressSTT && e.error !== 'not-allowed') {
                        setTimeout(() => {
                            if (STATE.isActive && !STATE.suppressSTT && !STATE.sttRunning) {
                                try { STATE.recognition.start(); } catch (err) { }
                            }
                        }, 1000);
                    }
                };

                STATE.recognition.onresult = (event) => {
                    // Discard STT results while AI is speaking (echo prevention)
                    if (STATE.isSpeaking) return;

                    const result = event.results[event.results.length - 1];
                    const text = result[0].transcript.trim();
                    const confidence = result[0].confidence || 1.0;

                    if (text.length > 0) {
                        DOM.subtitle.textContent = text;
                        DOM.subtitle.style.color = "white";
                        STATE.lastInterimText = text;
                    }

                    // FAST PATH: browser marked it final
                    if (result.isFinal && text.length > 1) {
                        clearSilenceTimer();
                        STATE.lastInterimText = '';
                        console.log(` Final: "${text}" | conf: ${confidence.toFixed(2)}`);
                        if (!isNoise(text, confidence)) {
                            sendToAI(text);
                        }
                        return;
                    }

                    // SILENCE TIMER: fire after pause in speech
                    clearSilenceTimer();
                    if (text.length > 1) {
                        STATE.silenceTimer = setTimeout(() => {
                            const captured = STATE.lastInterimText;
                            STATE.lastInterimText = '';
                            if (captured.length > 1 && !STATE.isSpeaking && !isNoise(captured, 1.0)) {
                                console.log(' Silence timer fired:', captured);
                                sendToAI(captured);
                            }
                        }, CONFIG.SILENCE_THRESHOLD_MS);
                    }
                };

                STATE.recognition.onend = () => {
                    STATE.sttRunning = false;
                    // Only restart if not suppressing (i.e. AI is not speaking)
                    if (STATE.isActive && !STATE.suppressSTT) {
                        try { STATE.recognition.start(); } catch (e) { }
                    }
                };

                STATE.recognition.start();

            } catch (err) {
                console.error("Audio Start Failure:", err);
                DOM.status.textContent = "System Error";
            }
        }

        // â”€â”€â”€ SEND TO AI (with cache + queue + history) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        let pendingQuery = null;
        let isAskingAI = false;

        async function sendToAI(text) {
            if (!STATE.isActive || !text.trim()) return;

            if (isAskingAI) {
                console.log(' Queued (busy):', text);
                pendingQuery = text;
                return;
            }

            isAskingAI = true;
            pendingQuery = null;
            console.log(' Sending to AI:', text);

            // âš¡ CACHE HIT: same question asked before â€” play instantly
            const key = cacheKey(text);
            const cached = responseCache.get(key);
            if (cached) {
                console.log(' Cache hit!', text);
                DOM.subtitle.textContent = cached.text;
                playVoice(cached.audios);
                isAskingAI = false;
                return;
            }

            DOM.status.textContent = "Thinking...";
            try {
                // Step 1: Get text from n8n (embed + search + LLM, no TTS)
                const res = await fetch(CONFIG.N8N_URL, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        question: text,
                        history: conversationHistory.slice(-MAX_HISTORY_TURNS)
                    })
                });
                const data = await res.json();
                const item = Array.isArray(data) ? data[0] : data;
                const responseText = item?.text || item?.output || item?.response || '';
                console.log(' n8n text:', responseText);

                if (!responseText) {
                    console.warn(' No text in response. Keys:', item ? Object.keys(item) : 'null');
                    DOM.status.textContent = "Connected";
                    DOM.subtitle.textContent = "I'm listening...";
                    return;
                }

                // Store in conversation history
                conversationHistory.push({ q: text, a: responseText });
                if (conversationHistory.length > MAX_HISTORY_TURNS) conversationHistory.shift();
                console.log(` History: ${conversationHistory.length} turns`);

                // Step 2: Call Sarvam TTS directly from browser
                DOM.status.textContent = "Speaking...";
                DOM.subtitle.textContent = responseText;
                const ttsRes = await fetch(CONFIG.SARVAM_URL, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${CONFIG.SARVAM_KEY}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        text: responseText,
                        language_code: 'en-IN',
                        speaker: 'arya',
                        format: 'mp3'
                    })
                });
                const ttsData = await ttsRes.json();
                console.log(' TTS keys:', Object.keys(ttsData));

                if (ttsData.audios && ttsData.audios.length > 0) {
                    // Cache for instant replay
                    responseCache.set(key, { text: responseText, audios: ttsData.audios });
                    if (responseCache.size > 30) responseCache.delete(responseCache.keys().next().value);
                    console.log(` Cached. Size: ${responseCache.size}`);
                    playVoice(ttsData.audios);
                } else {
                    console.warn(' No audio from Sarvam:', Object.keys(ttsData));
                    // Fallback: browser speech synthesis
                    const utt = new SpeechSynthesisUtterance(responseText);
                    utt.lang = 'en-IN';
                    utt.onend = () => {
                        STATE.isSpeaking = false;
                        resumeSTT();
                        DOM.status.textContent = "Connected";
                        DOM.subtitle.textContent = "I'm listening...";
                    };
                    STATE.isSpeaking = true;
                    STATE.aiStartTime = Date.now();
                    pauseSTT();
                    window.speechSynthesis.speak(utt);
                }
            } catch (e) {
                console.error("Error:", e);
                DOM.status.textContent = "Error";
                setTimeout(() => { DOM.status.textContent = "Connected"; }, 2000);
            } finally {
                isAskingAI = false;
                if (pendingQuery && STATE.isActive && !STATE.isSpeaking) {
                    const queued = pendingQuery;
                    pendingQuery = null;
                    console.log(' Sending queued:', queued);
                    setTimeout(() => sendToAI(queued), 200);
                }
            }
        }

        // â”€â”€â”€ PLAY AUDIO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        async function playVoice(chunks) {
            const ctx = getAudioCtx();
            interruptAI(); // clear any previous audio

            STATE.isSpeaking = true;
            STATE.aiStartTime = Date.now();
            DOM.status.textContent = "Speaking";
            DOM.subtitle.style.color = "var(--accent-color)";

            // KEY: Stop STT before playing so mic doesn't hear speaker
            pauseSTT();

            for (let b64 of chunks) {
                if (!STATE.isSpeaking) break;
                await new Promise(resolve => {
                    try {
                        const audio = new Audio("data:audio/mp3;base64," + (b64.includes(',') ? b64.split(',')[1] : b64));
                        const source = ctx.createMediaElementSource(audio);
                        source.connect(ctx.destination);

                        STATE.audioQueue.push(audio);
                        audio.onended = () => {
                            STATE.audioQueue = STATE.audioQueue.filter(a => a !== audio);
                            resolve();
                        };
                        audio.onerror = resolve;
                        audio.play().catch(resolve);
                    } catch (e) {
                        resolve();
                    }
                });
            }

            // All chunks played (or barge-in stopped it)
            if (STATE.isActive && STATE.audioQueue.length === 0) {
                STATE.isSpeaking = false;
                DOM.status.textContent = "Connected";
                DOM.subtitle.textContent = "I'm listening...";
                DOM.subtitle.style.color = "rgba(255,255,255,0.7)";

                // KEY: Resume STT after AI finishes
                resumeSTT();
            }
        }

        // â”€â”€â”€ CALL LIFECYCLE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        DOM.startBtn.addEventListener('click', () => {
            getAudioCtx();
            STATE.isActive = true;
            DOM.container.classList.add('active-call', 'dialing');
            DOM.startBtn.style.display = 'none';
            DOM.endBtn.style.display = 'flex';
            DOM.status.textContent = "Dialing...";

            startDialTone();

            setTimeout(() => {
                if (!STATE.isActive) return;
                stopDialTone();
                DOM.container.classList.remove('dialing');
                DOM.container.classList.add('connected');
                DOM.status.textContent = "Connected";
                DOM.subtitle.textContent = "Listening...";
                startListening();

                // Greet via Sarvam TTS directly â€” bypass n8n KB search
                const greetText = "Hello! I'm Sensi, your voice assistant. How can I help you today?";
                DOM.subtitle.textContent = greetText;
                STATE.isSpeaking = true;
                STATE.aiStartTime = Date.now();
                pauseSTT();
                fetch(CONFIG.SARVAM_URL, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${CONFIG.SARVAM_KEY}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ text: greetText, language_code: 'en-IN', speaker: 'arya', format: 'mp3' })
                }).then(r => r.json()).then(d => {
                    if (d.audios && d.audios.length > 0) {
                        playVoice(d.audios);
                    } else {
                        const utt = new SpeechSynthesisUtterance(greetText);
                        utt.lang = 'en-IN';
                        utt.onend = () => { STATE.isSpeaking = false; resumeSTT(); DOM.subtitle.textContent = "I'm listening..."; };
                        window.speechSynthesis.speak(utt);
                    }
                }).catch(() => { STATE.isSpeaking = false; resumeSTT(); });
            }, 3000);
        });

        DOM.endBtn.addEventListener('click', () => {
            STATE.isActive = false;
            STATE.suppressSTT = true;
            stopDialTone();
            interruptAI();
            clearSilenceTimer();

            if (STATE.recognition) {
                STATE.recognition.onend = null;
                STATE.recognition.stop();
            }

            if (STATE.micStream) {
                STATE.micStream.getTracks().forEach(track => track.stop());
            }

            STATE.sttRunning = false;
            STATE.analyser = null;
            STATE.micStream = null;

            DOM.container.classList.remove('active-call', 'dialing', 'connected');
            DOM.startBtn.style.display = 'flex';
            DOM.endBtn.style.display = 'none';
            DOM.status.textContent = "Standby";
            DOM.subtitle.textContent = "Ready to start";
            DOM.transcription.textContent = "";
            conversationHistory = [];
            responseCache.clear();
            console.log(' Session cleared');
        });
    </script>
</body>

</html>
